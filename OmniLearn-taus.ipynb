{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67701390",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bc0944-e298-4d08-b42e-8fcafda114fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"scripts\")\n",
    "import utils\n",
    "from PET import PET\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019bea2",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "BATCH_SIZE = 512\n",
    "LR = 1e-4\n",
    "\n",
    "NTRAIN = 200000\n",
    "NVAL = 100000\n",
    "\n",
    "OUTFILE = \"out/test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a64a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward\n",
    "import vector\n",
    "\n",
    "def to_p4(p4_obj):\n",
    "    return vector.awk(\n",
    "        awkward.zip(\n",
    "            {\n",
    "                \"mass\": p4_obj.tau,\n",
    "                \"x\": p4_obj.x,\n",
    "                \"y\": p4_obj.y,\n",
    "                \"z\": p4_obj.z,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "def deltaphi(phi1, phi2):\n",
    "    diff = phi1 - phi2\n",
    "    return np.arctan2(np.sin(diff), np.cos(diff))\n",
    "\n",
    "def deltar(eta1, phi1, eta2, phi2):\n",
    "    deta = eta1 - eta2\n",
    "    dphi = deltaphi(phi1, phi2)\n",
    "    return np.sqrt(deta**2 + dphi**2)\n",
    "\n",
    "#initialize weights of model1 from weights of model2.\n",
    "#ignore layers that don't match in shape.\n",
    "def set_matching_weights(model1, model2):\n",
    "    il1 = len(model1.layers)\n",
    "    il2 = len(model2.layers)\n",
    "    maxl = max(il1, il1)\n",
    "    for il in range(maxl):\n",
    "        if il<il1 and il<il2:\n",
    "            if len(model1.layers[il].weights)>0:\n",
    "                if len(model1.layers[il].weights) == len(model2.layers[il].weights):\n",
    "                    weights_match = True\n",
    "                    for w1, w2 in zip(model1.layers[il].weights, model2.layers[il].weights):\n",
    "                        if w1.shape != w2.shape:\n",
    "                            weights_match = False\n",
    "                    if weights_match:\n",
    "                        # print(model1.layers[il].name, model2.layers[il].name, [w.shape.as_list() for w in model1.layers[il].weights])\n",
    "                        model1.layers[il].set_weights(model2.layers[il].weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c642ee",
   "metadata": {},
   "source": [
    "Define the backbone model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6befe447-e662-4229-a30f-73902740eb4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define original model to get the weights\n",
    "model = PET(\n",
    "    num_feat=13,\n",
    "    num_jet=4,\n",
    "    num_classes=10, #this is the number of target classes the original model was trained with. not relevant for us.\n",
    "    local=True,\n",
    "    num_layers=8,\n",
    "    drop_probability=0,\n",
    "    simple=False,\n",
    "    layer_scale=True,\n",
    "    talking_head=False,\n",
    "    mode=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76249689",
   "metadata": {},
   "source": [
    "Run the backbone model on dummy data to initialize weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e701f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "x[\"input_features\"] = tf.zeros((32, 100, 13))\n",
    "x[\"input_points\"] = tf.zeros((32, 100, 2))\n",
    "x[\"input_mask\"] = tf.zeros((32, 100))\n",
    "x[\"input_jet\"] = tf.zeros((32, 4)) #how many features per jet\n",
    "x[\"input_time\"] = tf.zeros((32, 1)) #this is not used\n",
    "\n",
    "model(x)\n",
    "out = model.body(x)\n",
    "print(len(out), out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d69405f",
   "metadata": {},
   "source": [
    "Load the weights of the backbone model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca000ed4-2aa9-4a77-8de0-2a58b289eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.load_weights(\"checkpoints/PET_jetclass_8_local_layer_scale_token_baseline_all.weights.h5\", by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ae706",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = awkward.from_parquet(\"zh.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_cand_p4s = to_p4(data[\"reco_cand_p4s\"])\n",
    "reco_jet_p4s = to_p4(data[\"reco_jet_p4s\"])\n",
    "\n",
    "delta_eta = reco_cand_p4s.eta - reco_jet_p4s.eta\n",
    "delta_phi = deltaphi(reco_cand_p4s.phi, reco_jet_p4s.phi)\n",
    "log_pt = np.log(reco_cand_p4s.pt)\n",
    "log_e = np.log(reco_cand_p4s.energy)\n",
    "log_ptjet = np.log(1 - reco_cand_p4s.pt/reco_jet_p4s.pt)\n",
    "log_ejet = np.log(1 - reco_cand_p4s.energy/reco_jet_p4s.energy)\n",
    "delta_r = deltar(reco_cand_p4s.eta, reco_cand_p4s.phi, reco_jet_p4s.eta, reco_jet_p4s.phi)\n",
    "charge = data[\"reco_cand_charge\"]\n",
    "is_ele = np.abs(data[\"reco_cand_pdg\"])==11\n",
    "is_mu = np.abs(data[\"reco_cand_pdg\"])==13\n",
    "is_photon = np.abs(data[\"reco_cand_pdg\"])==22\n",
    "is_chhad = np.abs(data[\"reco_cand_pdg\"])==211\n",
    "is_nhad = np.abs(data[\"reco_cand_pdg\"])==130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16610b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(awkward.num(reco_cand_p4s.pt), bins=np.linspace(0,32,33));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e7a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_size = 32 #max number of particles per jet\n",
    "fill_val = 0 #fill value of padded data\n",
    "\n",
    "#create particle array in the shape [njets, pad_size, 13]\n",
    "vals = [\n",
    "    awkward.to_numpy(\n",
    "        awkward.fill_none(\n",
    "            awkward.pad_none(\n",
    "                x, pad_size, clip=True), fill_val\n",
    "        )\n",
    "    ) for x in [delta_eta, delta_phi, log_ptjet, log_pt, log_ejet, log_e , delta_r, charge, is_chhad, is_nhad, is_photon, is_ele, is_mu]\n",
    "]\n",
    "particles = np.stack(vals, axis=-1)\n",
    "particles[np.isnan(particles)] = 0\n",
    "particles[np.isinf(particles)] = 0\n",
    "particles_mask = (~awkward.to_numpy(awkward.pad_none(delta_eta, pad_size, clip=True)).mask).astype(np.float32)\n",
    "\n",
    "#normalize particles\n",
    "means_particle = particles[np.squeeze(particles_mask==1)].mean(axis=0)\n",
    "means_particle[7:] = 0\n",
    "stds_particle = particles[np.squeeze(particles_mask==1)].std(axis=0)\n",
    "stds_particle[7:] = 1\n",
    "stds_particle[stds_particle==0] = 1\n",
    "particles = (particles - means_particle)/stds_particle\n",
    "\n",
    "#create jet array in the shape [njets, 4]\n",
    "jets = awkward.to_numpy(np.stack([\n",
    "    reco_jet_p4s.pt,\n",
    "    reco_jet_p4s.eta,\n",
    "    reco_jet_p4s.mass,\n",
    "    awkward.num(reco_cand_p4s)], axis=-1)\n",
    ")\n",
    "jets[np.isnan(jets)] = 0\n",
    "jets[np.isinf(jets)] = 0\n",
    "\n",
    "#normalize jets\n",
    "means_jet = jets.mean(axis=0)\n",
    "stds_jet = jets.std(axis=0)\n",
    "stds_jet[stds_jet==0] = 1\n",
    "jets = (jets - means_jet)/stds_jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659799c8-d572-44e5-8168-892c5cd6052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "particles[np.squeeze(particles_mask==1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7381908",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_particle, stds_particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_jet, stds_jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4486af",
   "metadata": {},
   "outputs": [],
   "source": [
    "particles.shape, particles_mask.shape, jets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_dm = awkward.to_numpy(data[\"gen_jet_tau_decaymode\"])\n",
    "targets_pt = np.log(to_p4(data[\"gen_jet_tau_p4s\"]).pt/to_p4(data[\"reco_jet_p4s\"]).pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc147e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(start, stop):\n",
    "    x = {}\n",
    "    x[\"input_features\"] = particles[start:stop]\n",
    "    x[\"input_points\"] = particles[start:stop, :, :2]\n",
    "    x[\"input_mask\"] = np.expand_dims(particles_mask[start:stop], axis=-1)\n",
    "    x[\"input_jet\"] = jets[start:stop]\n",
    "    x[\"input_time\"] = np.zeros((stop-start, 1))\n",
    "    y_dm = awkward.to_numpy(targets_dm[start:stop])\n",
    "    y_pt = awkward.to_numpy(targets_pt[start:stop])\n",
    "    return x, y_dm, y_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from layers import StochasticDepth, TalkingHeadAttention, LayerScale, RandomDrop\n",
    "\n",
    "def get_encoding(x, projection_dim, use_bias=True):\n",
    "    x = layers.Dense(2*projection_dim, use_bias=use_bias, activation='gelu')(x)\n",
    "    x = layers.Dense(projection_dim, use_bias=use_bias, activation='gelu')(x)\n",
    "    return x\n",
    "\n",
    "def FourierProjection(x,projection_dim,num_embed=64):    \n",
    "    half_dim = num_embed // 2\n",
    "    emb = tf.math.log(10000.0) / (half_dim - 1)\n",
    "    emb = tf.cast(emb,tf.float32)\n",
    "    freq = tf.exp(-emb* tf.range(start=0, limit=half_dim, dtype=tf.float32))\n",
    "\n",
    "\n",
    "    angle = x*freq*1000.0\n",
    "    embedding = tf.concat([tf.math.sin(angle),tf.math.cos(angle)],-1)*x\n",
    "    embedding = layers.Dense(2*projection_dim,activation=\"swish\",use_bias=False)(embedding)\n",
    "    embedding = layers.Dense(projection_dim,activation=\"swish\",use_bias=False)(embedding)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def knn(num_points, k, topk_indices, features):\n",
    "    # topk_indices: (N, P, K)\n",
    "    # features: (N, P, C)    \n",
    "    batch_size = tf.shape(features)[0]\n",
    "\n",
    "    batch_indices = tf.reshape(tf.range(batch_size), (-1, 1, 1))\n",
    "    batch_indices = tf.tile(batch_indices, (1, num_points, k))\n",
    "    indices = tf.stack([batch_indices, topk_indices], axis=-1)\n",
    "    return tf.gather_nd(features, indices)\n",
    "\n",
    "def get_neighbors(points,features,projection_dim,K):\n",
    "    drij = pairwise_distance(points)  # (N, P, P)\n",
    "    _, indices = tf.nn.top_k(-drij, k=K + 1)  # (N, P, K+1)\n",
    "    indices = indices[:, :, 1:]  # (N, P, K)\n",
    "    knn_fts = knn(tf.shape(points)[1], K, indices, features)  # (N, P, K, C)\n",
    "    knn_fts_center = tf.broadcast_to(tf.expand_dims(features, 2), tf.shape(knn_fts))\n",
    "    local = tf.concat([knn_fts-knn_fts_center,knn_fts_center],-1)\n",
    "    local = layers.Dense(2*projection_dim,activation='gelu')(local)\n",
    "    local = layers.Dense(projection_dim,activation='gelu')(local)\n",
    "    local = tf.reduce_mean(local,-2)\n",
    "    \n",
    "    return local\n",
    "\n",
    "def pairwise_distance(point_cloud):\n",
    "    r = tf.reduce_sum(point_cloud * point_cloud, axis=2, keepdims=True)\n",
    "    m = tf.matmul(point_cloud, point_cloud, transpose_b = True)\n",
    "    D = r - 2 * m + tf.transpose(r, perm=(0, 2, 1)) + 1e-5\n",
    "    return D\n",
    "\n",
    "\n",
    "class TransformerModel(keras.Model):\n",
    "    def __init__(self,\n",
    "                 use_backbone,\n",
    "                 num_feat,\n",
    "                 num_jet,      \n",
    "                 num_classes=2):\n",
    "        \n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.projection_dim = model.projection_dim\n",
    "        self.num_heads = model.num_heads\n",
    "        self.num_classes = num_classes\n",
    "        self.class_activation = \"softmax\"\n",
    "        \n",
    "        self.feature_drop = model.feature_drop\n",
    "        self.num_keep = model.num_keep\n",
    "        self.mode = model.mode\n",
    "        self.num_layers = model.num_layers\n",
    "        self.layer_scale = model.layer_scale\n",
    "        self.layer_scale_init = model.layer_scale_init\n",
    "        self.drop_probability = model.drop_probability\n",
    "        self.dropout = model.dropout\n",
    "        self.num_class_layers = 2\n",
    "        \n",
    "        self._input_features = layers.Input(shape=(None, num_feat), name='input_features')\n",
    "        self._input_points = layers.Input(shape=(None, 2), name='input_points')\n",
    "        self._input_mask = layers.Input(shape=(None, 1), name='input_mask')\n",
    "        self._input_jet = layers.Input((num_jet, ),name='input_jet')\n",
    "        self._input_time = layers.Input((None, ),name='input_time')\n",
    "\n",
    "        if use_backbone:\n",
    "            self.backbone_body = self.PET_body(\n",
    "                self._input_features,\n",
    "                self._input_points,\n",
    "                self._input_mask,\n",
    "                self._input_time,\n",
    "                True,\n",
    "                self.num_classes,\n",
    "                2,\n",
    "                False\n",
    "            )\n",
    "            self.backbone = keras.Model(\n",
    "                inputs=[self._input_features, self._input_points, self._input_mask, self._input_time],\n",
    "                outputs=[self.backbone_body], name=\"backbone\"\n",
    "            )\n",
    "            particles_encoded = self.backbone_body\n",
    "        else:\n",
    "            particles_encoded = get_encoding(self._input_features, self.projection_dim)\n",
    "\n",
    "        classifier_out, regression_out = self.PET_classifier(\n",
    "            particles_encoded, self._input_jet, self.num_class_layers, 1\n",
    "        )\n",
    "        self.classifier_and_regression = keras.Model(\n",
    "            inputs=[self._input_features, self._input_points, self._input_mask, self._input_jet, self._input_time],\n",
    "            outputs=[classifier_out, regression_out], name=\"classifier_and_regression\"\n",
    "        )\n",
    "            \n",
    "    def PET_classifier(\n",
    "            self,\n",
    "            encoded,\n",
    "            input_jet,\n",
    "            num_class_layers,\n",
    "            num_jet,\n",
    "            simple = False\n",
    "    ):\n",
    "\n",
    "        #Include event information as a representative particle\n",
    "        if simple:\n",
    "            encoded = layers.GroupNormalization(groups=1)(encoded)\n",
    "            representation = layers.GlobalAveragePooling1D()(encoded)\n",
    "            jet_encoded = get_encoding(input_jet,self.projection_dim)\n",
    "            representation = layers.Dense(self.projection_dim,activation='gelu')(representation+jet_encoded)\n",
    "            outputs_pred = layers.Dense(self.num_classes,activation=self.class_activation)(representation)\n",
    "            outputs_mse = layers.Dense(num_jet)(representation)\n",
    "        else:\n",
    "            conditional = layers.Dense(2*self.projection_dim,activation='gelu')(input_jet)\n",
    "            conditional = tf.tile(conditional[:,None, :], [1,tf.shape(encoded)[1], 1])\n",
    "            scale,shift = tf.split(conditional,2,-1)\n",
    "            encoded = encoded*(1.0 + scale) + shift\n",
    "\n",
    "            class_tokens = tf.Variable(tf.zeros(shape=(1, self.projection_dim)),trainable = True)    \n",
    "            class_tokens = tf.tile(class_tokens[None, :, :], [tf.shape(encoded)[0], 1, 1])\n",
    "                        \n",
    "            for _ in range(num_class_layers):\n",
    "                concatenated = tf.concat([class_tokens, encoded],1)\n",
    "\n",
    "                x1 = layers.GroupNormalization(groups=1)(concatenated)            \n",
    "                updates = layers.MultiHeadAttention(num_heads=self.num_heads,\n",
    "                                                    key_dim=self.projection_dim//self.num_heads)(\n",
    "                                                        query=x1[:,:1], value=x1, key=x1)\n",
    "                updates = layers.GroupNormalization(groups=1)(updates)\n",
    "                if self.layer_scale:\n",
    "                    updates = LayerScale(self.layer_scale_init, self.projection_dim)(updates)\n",
    "\n",
    "                x2 = layers.Add()([updates,class_tokens])\n",
    "                x3 = layers.GroupNormalization(groups=1)(x2)\n",
    "                x3 = layers.Dense(2*self.projection_dim,activation=\"gelu\")(x3)\n",
    "                x3 = layers.Dropout(self.dropout)(x3)\n",
    "                x3 = layers.Dense(self.projection_dim)(x3)\n",
    "                if self.layer_scale:\n",
    "                    x3 = LayerScale(self.layer_scale_init, self.projection_dim)(x3)\n",
    "                class_tokens = layers.Add()([x3,x2])\n",
    "\n",
    "\n",
    "            class_tokens = layers.GroupNormalization(groups=1)(class_tokens)\n",
    "            outputs_pred = layers.Dense(self.num_classes,activation=self.class_activation)(class_tokens[:,0])\n",
    "            outputs_mse = layers.Dense(num_jet)(class_tokens[:,0])\n",
    "\n",
    "        return outputs_pred, outputs_mse\n",
    "    \n",
    "    def PET_body(self,\n",
    "                 input_features,\n",
    "                 input_points,\n",
    "                 input_mask,\n",
    "                 input_time,\n",
    "                 local, K,num_local,\n",
    "                 talking_head,\n",
    "                 ):\n",
    "            \n",
    "        #Randomly drop features not present in other datasets\n",
    "        encoded = RandomDrop(self.feature_drop if  'all' in self.mode else 0.0,num_skip=self.num_keep)(input_features)                        \n",
    "        encoded = get_encoding(encoded,self.projection_dim)\n",
    "\n",
    "        time = FourierProjection(input_time,self.projection_dim)\n",
    "        time = tf.tile(time[:,None, :], [1,tf.shape(encoded)[1], 1])*input_mask\n",
    "        time = layers.Dense(2*self.projection_dim,activation='gelu',use_bias=False)(time)\n",
    "        scale,shift = tf.split(time,2,-1)\n",
    "        \n",
    "        encoded = encoded*(1.0+scale) + shift\n",
    "        \n",
    "        if local:\n",
    "            coord_shift = tf.multiply(999., tf.cast(tf.equal(input_mask, 0), dtype='float32'))        \n",
    "            points = input_points[:,:,:2]\n",
    "            local_features = input_features\n",
    "            for _ in range(num_local):\n",
    "                local_features = get_neighbors(coord_shift+points,local_features,self.projection_dim,K)\n",
    "                points = local_features\n",
    "                \n",
    "            encoded = layers.Add()([local_features,encoded])\n",
    "\n",
    "        skip_connection = encoded\n",
    "        for i in range(self.num_layers):\n",
    "            x1 = layers.GroupNormalization(groups=1)(encoded)\n",
    "            if talking_head:\n",
    "                updates, _ = TalkingHeadAttention(self.projection_dim, self.num_heads, 0.0)(x1)\n",
    "            else:\n",
    "                updates = layers.MultiHeadAttention(num_heads=self.num_heads,\n",
    "                                                    key_dim=self.projection_dim//self.num_heads)(x1,x1)\n",
    "\n",
    "            if self.layer_scale:\n",
    "                updates = LayerScale(self.layer_scale_init, self.projection_dim)(updates,input_mask)\n",
    "            updates = StochasticDepth(self.drop_probability)(updates)\n",
    "            x2 = layers.Add()([updates,encoded])\n",
    "            x3 = layers.GroupNormalization(groups=1)(x2)\n",
    "            x3 = layers.Dense(2*self.projection_dim,activation=\"gelu\")(x3)\n",
    "            x3 = layers.Dropout(self.dropout)(x3)\n",
    "            x3 = layers.Dense(self.projection_dim)(x3)\n",
    "            if self.layer_scale:\n",
    "                x3 = LayerScale(self.layer_scale_init, self.projection_dim)(x3,input_mask)\n",
    "            x3 = StochasticDepth(self.drop_probability)(x3)\n",
    "            encoded = layers.Add()([x3,x2])*input_mask\n",
    "        return encoded + skip_connection\n",
    "    \n",
    "    def call(self, x):\n",
    "        ret = self.classifier_and_regression([\n",
    "            x[\"input_features\"], x[\"input_points\"], x[\"input_mask\"], x[\"input_jet\"], x[\"input_time\"]\n",
    "        ])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_dm_val, y_pt_val = prepare_data(NTRAIN, NTRAIN+NVAL)\n",
    "\n",
    "#one small test batch\n",
    "x, y_dm, y_pt = prepare_data(0, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb65c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[\"input_features\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8cf5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "histories_direct = []\n",
    "histories_bb = []\n",
    "histories_bb_cp = []\n",
    "\n",
    "X_train, y_dm_train, y_pt_train = prepare_data(0, NTRAIN)\n",
    "\n",
    "#model without backbone\n",
    "model_dm_direct = TransformerModel(False, 13, 4, 16)\n",
    "model_dm_direct(x)\n",
    "model_dm_direct.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss=[keras.losses.SparseCategoricalCrossentropy(from_logits=False), keras.losses.MeanAbsoluteError()]\n",
    ")\n",
    "\n",
    "#model with backbone, initialization from scratch\n",
    "model_dm_bb = TransformerModel(True, 13, 4, 16)\n",
    "model_dm_bb(x)\n",
    "model_dm_bb.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss=[keras.losses.SparseCategoricalCrossentropy(from_logits=False), keras.losses.MeanAbsoluteError()]\n",
    ")\n",
    "\n",
    "#model with backbone, initialize from checkpoint\n",
    "model_dm_bb_cp = TransformerModel(True, 13, 4, 16)\n",
    "model_dm_bb_cp(x)\n",
    "model_dm_bb_cp.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss=[keras.losses.SparseCategoricalCrossentropy(from_logits=False), keras.losses.MeanAbsoluteError()]\n",
    ")\n",
    "\n",
    "#set backbone weights\n",
    "set_matching_weights(model_dm_bb_cp.backbone, model.body)\n",
    "set_matching_weights(model_dm_bb_cp.classifier_and_regression, model.classifier)\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(patience=20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03962f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"training direct\")\n",
    "history_direct = model_dm_direct.fit(\n",
    "    X_train, (y_dm_train, y_pt_train),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, (y_dm_val, y_pt_val)),\n",
    "    verbose=2,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "print(\"direct val_loss={:.2f}\".format(history_direct.history[\"val_loss\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb7de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history_direct.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history_direct.history[\"val_loss\"], label=\"val\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee844e58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"training backbone\")\n",
    "history_bb = model_dm_bb.fit(\n",
    "    X_train, (y_dm_train, y_pt_train),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, (y_dm_val, y_pt_val)),\n",
    "    verbose=2,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "print(\"backbone val_loss={:.2f}\".format(history_bb.history[\"val_loss\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463972c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history_bb.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history_bb.history[\"val_loss\"], label=\"val\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training backbone checkpoint\")\n",
    "history_bb_cp = model_dm_bb_cp.fit(\n",
    "    X_train, (y_dm_train, y_pt_train),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, (y_dm_val, y_pt_val)),\n",
    "    verbose=2,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "print(\"backbone checkpoint val_loss={:.2f}\".format(history_bb_cp.history[\"val_loss\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history_bb_cp.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history_bb_cp.history[\"val_loss\"], label=\"val\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89538e1b-e224-44db-8e8c-dbf9eaa8b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Ntrain={}\".format(NTRAIN))\n",
    "plt.plot(history_direct.history[\"val_loss\"], label=\"no backbone\")\n",
    "plt.plot(history_bb.history[\"val_loss\"], label=\"OmniLearn naive\")\n",
    "plt.plot(history_bb_cp.history[\"val_loss\"], label=\"OmniLearn checkpoint\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"validation loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfc1486",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OUTFILE, \"w\") as fi:\n",
    "    json.dump({\n",
    "        \"BATCH_SIZE\": BATCH_SIZE,\n",
    "        \"NTRAIN\": NTRAIN,\n",
    "        \"NVAL\": NVAL,\n",
    "        \"history_direct\": history_direct.history,\n",
    "        \"history_bb\": history_bb.history,\n",
    "        \"history_bb_cp\": history_bb_cp.history,\n",
    "    }, fi, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf449a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dm_probas1, pred_pt1 = model_dm_direct.predict(X_val, batch_size=BATCH_SIZE)\n",
    "pred_dm_probas2, pred_pt2 = model_dm_bb.predict(X_val, batch_size=BATCH_SIZE)\n",
    "pred_dm_probas3, pred_pt3 = model_dm_bb_cp.predict(X_val, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dm1 = tf.argmax(pred_dm_probas1, axis=-1)\n",
    "pred_dm2 = tf.argmax(pred_dm_probas2, axis=-1)\n",
    "pred_dm3 = tf.argmax(pred_dm_probas3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a25f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sklearn.metrics.confusion_matrix(y_dm_val, pred_dm1, labels=range(16)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e09914",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "b = np.linspace(-0.2,0.2,100)\n",
    "plt.hist2d(\n",
    "    y_pt_val,\n",
    "    pred_pt1[:, 0],\n",
    "    bins=(b, b), cmap=\"Blues\"\n",
    ");\n",
    "plt.xlabel(\"target log(genpt/recopt)\")\n",
    "plt.ylabel(\"predicted log(genpt/recopt)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "b = np.linspace(0,200,100)\n",
    "plt.hist2d(\n",
    "    awkward.to_numpy(to_p4(data[\"gen_jet_p4s\"][NTRAIN:NTRAIN+NVAL]).pt),\n",
    "    awkward.to_numpy(np.exp(pred_pt1[:, 0]) * to_p4(data[\"reco_jet_p4s\"][NTRAIN:NTRAIN+NVAL]).pt),\n",
    "    bins=(b, b), cmap=\"Blues\"\n",
    ");\n",
    "plt.plot([0,200],[0,200])\n",
    "plt.xlabel(\"true pt\")\n",
    "plt.ylabel(\"predicted pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89457fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "b = np.linspace(0.75, 1.25, 200)\n",
    "\n",
    "plt.hist(\n",
    "    awkward.to_numpy(to_p4(data[\"reco_jet_p4s\"][NTRAIN:NTRAIN+NVAL]).pt)/awkward.to_numpy(to_p4(data[\"gen_jet_p4s\"][NTRAIN:NTRAIN+NVAL]).pt),\n",
    "    bins=b,\n",
    "    histtype=\"step\", lw=1, label=\"raw recojet\", density=1\n",
    ");\n",
    "\n",
    "plt.hist(\n",
    "    awkward.to_numpy(np.exp(pred_pt1[:, 0]) * to_p4(data[\"reco_jet_p4s\"][NTRAIN:NTRAIN+NVAL]).pt)/awkward.to_numpy(to_p4(data[\"gen_jet_p4s\"][NTRAIN:NTRAIN+NVAL]).pt),\n",
    "    bins=b,\n",
    "    histtype=\"step\", lw=1, label=\"direct\", density=1\n",
    ");\n",
    "\n",
    "plt.hist(\n",
    "    awkward.to_numpy(np.exp(pred_pt2[:, 0]) * to_p4(data[\"reco_jet_p4s\"][NTRAIN:NTRAIN+NVAL]).pt)/awkward.to_numpy(to_p4(data[\"gen_jet_p4s\"][NTRAIN:NTRAIN+NVAL]).pt),\n",
    "    bins=b,\n",
    "    histtype=\"step\", lw=1, label=\"OmniLearn naive\", density=1\n",
    ");\n",
    "\n",
    "plt.hist(\n",
    "    awkward.to_numpy(np.exp(pred_pt3[:, 0]) * to_p4(data[\"reco_jet_p4s\"][NTRAIN:NTRAIN+NVAL]).pt)/awkward.to_numpy(to_p4(data[\"gen_jet_p4s\"][NTRAIN:NTRAIN+NVAL]).pt),\n",
    "    bins=b,\n",
    "    histtype=\"step\", lw=1, label=\"OmniLearn checkpoint\", density=1\n",
    ");\n",
    "\n",
    "plt.axvline(1.0, color=\"black\", ls=\"--\", lw=1.0)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"tau pred / true pt\")\n",
    "plt.ylim(top=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701e076e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bc0944-e298-4d08-b42e-8fcafda114fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"scripts\")\n",
    "import utils\n",
    "from PET import PET\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a64a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward\n",
    "import vector\n",
    "\n",
    "def to_p4(p4_obj):\n",
    "    return vector.awk(\n",
    "        awkward.zip(\n",
    "            {\n",
    "                \"mass\": p4_obj.tau,\n",
    "                \"x\": p4_obj.x,\n",
    "                \"y\": p4_obj.y,\n",
    "                \"z\": p4_obj.z,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "def deltaphi(phi1, phi2):\n",
    "    diff = phi1 - phi2\n",
    "    return np.arctan2(np.sin(diff), np.cos(diff))\n",
    "\n",
    "def deltar(eta1, phi1, eta2, phi2):\n",
    "    deta = eta1 - eta2\n",
    "    dphi = deltaphi(phi1, phi2)\n",
    "    return np.sqrt(deta**2 + dphi**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c642ee",
   "metadata": {},
   "source": [
    "Define the backbone model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6befe447-e662-4229-a30f-73902740eb4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PET(\n",
    "    num_feat=13,\n",
    "    num_jet=4,\n",
    "    num_classes=10,\n",
    "    local=True,\n",
    "    num_layers=8,\n",
    "    drop_probability=0,\n",
    "    simple=False,\n",
    "    layer_scale=True,\n",
    "    talking_head=False,\n",
    "    mode=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76249689",
   "metadata": {},
   "source": [
    "Run the backbone model on dummy data to initialize weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e701f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "x[\"input_features\"] = tf.zeros((32, 100, 13))\n",
    "x[\"input_points\"] = tf.zeros((32, 100, 2))\n",
    "x[\"input_mask\"] = tf.zeros((32, 100))\n",
    "x[\"input_jet\"] = tf.zeros((32, 4))\n",
    "x[\"input_time\"] = tf.zeros((32, 1))\n",
    "\n",
    "model(x)\n",
    "out = model.body(x)\n",
    "print(len(out), out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d69405f",
   "metadata": {},
   "source": [
    "Load the weights of the backbone model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca000ed4-2aa9-4a77-8de0-2a58b289eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.load_weights(\"checkpoints/PET_jetclass_8_local_layer_scale_token_baseline_all.weights.h5\", by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ae706",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = awkward.from_parquet(\"zh.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_cand_p4s = to_p4(data[\"reco_cand_p4s\"])\n",
    "reco_jet_p4s = to_p4(data[\"reco_jet_p4s\"])\n",
    "\n",
    "delta_eta = reco_cand_p4s.eta - reco_jet_p4s.eta\n",
    "delta_phi = deltaphi(reco_cand_p4s.phi, reco_jet_p4s.phi)\n",
    "log_pt = np.log(reco_cand_p4s.pt)\n",
    "log_e = np.log(reco_cand_p4s.energy)\n",
    "log_ptjet = np.log(1 - reco_cand_p4s.pt/reco_jet_p4s.pt)\n",
    "log_ejet = np.log(1 - reco_cand_p4s.energy/reco_jet_p4s.energy)\n",
    "delta_r = deltar(reco_cand_p4s.eta, reco_cand_p4s.phi, reco_jet_p4s.eta, reco_jet_p4s.phi)\n",
    "charge = data[\"reco_cand_charge\"]\n",
    "is_ele = np.abs(data[\"reco_cand_pdg\"])==11\n",
    "is_mu = np.abs(data[\"reco_cand_pdg\"])==13\n",
    "is_photon = np.abs(data[\"reco_cand_pdg\"])==22\n",
    "is_chhad = np.abs(data[\"reco_cand_pdg\"])==210\n",
    "is_nhad = np.abs(data[\"reco_cand_pdg\"])==130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e7a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_size = 64 #max number of particles per jet\n",
    "fill_val = 0 #fill value of padded data\n",
    "\n",
    "#create particle array in the shape [njets, pad_size, 13]\n",
    "vals = [\n",
    "    awkward.to_numpy(\n",
    "        awkward.fill_none(\n",
    "            awkward.pad_none(\n",
    "                x, pad_size, clip=True), fill_val\n",
    "        )\n",
    "    ) for x in [delta_eta, delta_phi, log_pt, log_e, log_ptjet, log_ejet, delta_r, charge, is_ele, is_mu, is_photon, is_chhad, is_nhad]\n",
    "]\n",
    "particles = np.stack(vals, axis=-1)\n",
    "particles[np.isnan(particles)] = 0\n",
    "particles[np.isinf(particles)] = 0\n",
    "particles_mask = (~awkward.to_numpy(awkward.pad_none(delta_eta, pad_size, clip=True)).mask).astype(np.float32)\n",
    "\n",
    "#normalize particles\n",
    "means_particle = particles[np.squeeze(particles_mask==1)].mean(axis=0)\n",
    "stds_particle = particles[np.squeeze(particles_mask==1)].std(axis=0)\n",
    "stds_particle[stds_particle==0] = 1\n",
    "particles = (particles - means_particle)/stds_particle\n",
    "\n",
    "#create jet array in the shape [njets, 4]\n",
    "jets = awkward.to_numpy(np.stack([\n",
    "    reco_jet_p4s.pt,\n",
    "    reco_jet_p4s.eta,\n",
    "    reco_jet_p4s.mass,\n",
    "    awkward.num(reco_cand_p4s)], axis=-1)\n",
    ")\n",
    "jets[np.isnan(jets)] = 0\n",
    "jets[np.isinf(jets)] = 0\n",
    "\n",
    "#normalize jets\n",
    "means_jet = jets.mean(axis=0)\n",
    "stds_jet = jets.std(axis=0)\n",
    "stds_jet[stds_jet==0] = 1\n",
    "jets = (jets - means_jet)/stds_jet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7381908",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_particle, stds_particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_jet, stds_jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4486af",
   "metadata": {},
   "outputs": [],
   "source": [
    "particles.shape, particles_mask.shape, jets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = awkward.to_numpy(data[\"gen_jet_tau_decaymode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc147e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(start, stop):\n",
    "    x = {}\n",
    "    x[\"input_features\"] = particles[start:stop]\n",
    "    x[\"input_points\"] = particles[start:stop, :, :2]\n",
    "    x[\"input_mask\"] = np.expand_dims(particles_mask[start:stop], axis=-1)\n",
    "    x[\"input_jet\"] = jets[start:stop]\n",
    "    x[\"input_time\"] = np.zeros((stop-start, 1))\n",
    "    y = targets[start:stop]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from layers import StochasticDepth, TalkingHeadAttention, LayerScale, RandomDrop\n",
    "\n",
    "def get_encoding(x, projection_dim, use_bias=True):\n",
    "    x = layers.Dense(2*projection_dim, use_bias=use_bias, activation='gelu')(x)\n",
    "    x = layers.Dense(projection_dim, use_bias=use_bias, activation='gelu')(x)\n",
    "    return x\n",
    "\n",
    "def FourierProjection(x,projection_dim,num_embed=64):    \n",
    "    half_dim = num_embed // 2\n",
    "    emb = tf.math.log(10000.0) / (half_dim - 1)\n",
    "    emb = tf.cast(emb,tf.float32)\n",
    "    freq = tf.exp(-emb* tf.range(start=0, limit=half_dim, dtype=tf.float32))\n",
    "\n",
    "\n",
    "    angle = x*freq*1000.0\n",
    "    embedding = tf.concat([tf.math.sin(angle),tf.math.cos(angle)],-1)*x\n",
    "    embedding = layers.Dense(2*projection_dim,activation=\"swish\",use_bias=False)(embedding)\n",
    "    embedding = layers.Dense(projection_dim,activation=\"swish\",use_bias=False)(embedding)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def knn(num_points, k, topk_indices, features):\n",
    "    # topk_indices: (N, P, K)\n",
    "    # features: (N, P, C)    \n",
    "    batch_size = tf.shape(features)[0]\n",
    "\n",
    "    batch_indices = tf.reshape(tf.range(batch_size), (-1, 1, 1))\n",
    "    batch_indices = tf.tile(batch_indices, (1, num_points, k))\n",
    "    indices = tf.stack([batch_indices, topk_indices], axis=-1)\n",
    "    return tf.gather_nd(features, indices)\n",
    "\n",
    "def get_neighbors(points,features,projection_dim,K):\n",
    "    drij = pairwise_distance(points)  # (N, P, P)\n",
    "    _, indices = tf.nn.top_k(-drij, k=K + 1)  # (N, P, K+1)\n",
    "    indices = indices[:, :, 1:]  # (N, P, K)\n",
    "    knn_fts = knn(tf.shape(points)[1], K, indices, features)  # (N, P, K, C)\n",
    "    knn_fts_center = tf.broadcast_to(tf.expand_dims(features, 2), tf.shape(knn_fts))\n",
    "    local = tf.concat([knn_fts-knn_fts_center,knn_fts_center],-1)\n",
    "    local = layers.Dense(2*projection_dim,activation='gelu')(local)\n",
    "    local = layers.Dense(projection_dim,activation='gelu')(local)\n",
    "    local = tf.reduce_mean(local,-2)\n",
    "    \n",
    "    return local\n",
    "\n",
    "def pairwise_distance(point_cloud):\n",
    "    r = tf.reduce_sum(point_cloud * point_cloud, axis=2, keepdims=True)\n",
    "    m = tf.matmul(point_cloud, point_cloud, transpose_b = True)\n",
    "    D = r - 2 * m + tf.transpose(r, perm=(0, 2, 1)) + 1e-5\n",
    "    return D\n",
    "\n",
    "\n",
    "class TransformerModel(keras.Model):\n",
    "    def __init__(self,\n",
    "                 use_backbone,\n",
    "                 num_feat,\n",
    "                 num_jet,      \n",
    "                 num_classes=2):\n",
    "        \n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.projection_dim = model.projection_dim\n",
    "        self.num_heads = model.num_heads\n",
    "        \n",
    "        self.feature_drop = model.feature_drop\n",
    "        self.num_keep = model.num_keep\n",
    "        self.mode = model.mode\n",
    "        self.num_layers = model.num_layers\n",
    "        self.layer_scale = model.layer_scale\n",
    "        self.layer_scale_init = model.layer_scale_init\n",
    "        self.drop_probability = model.drop_probability\n",
    "        self.dropout = model.dropout\n",
    "        \n",
    "        self._input_features = layers.Input(shape=(None, num_feat), name='input_features')\n",
    "        self._input_points = layers.Input(shape=(None, 2), name='input_points')\n",
    "        self._input_mask = layers.Input(shape=(None, 1), name='input_mask')\n",
    "        self._input_jet = layers.Input((num_jet, ),name='input_jet')\n",
    "        self._input_time = layers.Input((None, ),name='input_time')\n",
    "\n",
    "        if use_backbone:\n",
    "            self.backbone_body = self.PET_body(\n",
    "                self._input_features,\n",
    "                self._input_points,\n",
    "                self._input_mask,\n",
    "                self._input_time,\n",
    "                True,\n",
    "                10,\n",
    "                2,\n",
    "                False\n",
    "            )\n",
    "            self.backbone = keras.Model(\n",
    "                inputs=[self._input_features, self._input_points, self._input_mask, self._input_time],\n",
    "                outputs=[self.backbone_body], name=\"backbone\"\n",
    "            )\n",
    "\n",
    "            particles_encoded = self.backbone_body\n",
    "        else:\n",
    "            particles_encoded = get_encoding(self._input_features, self.projection_dim)\n",
    "\n",
    "        for ilayer in range(2):\n",
    "            updates = layers.MultiHeadAttention(\n",
    "                num_heads=self.num_heads,\n",
    "                key_dim=self.projection_dim//self.num_heads)(\n",
    "                query=particles_encoded, value=particles_encoded, key=particles_encoded)\n",
    "            particles_encoded = layers.Add()([updates, particles_encoded])\n",
    "            particles_encoded = layers.GroupNormalization(groups=1)(particles_encoded)\n",
    "            particles_encoded = layers.Dense(self.projection_dim)(particles_encoded)\n",
    "        \n",
    "        representation = layers.GlobalAveragePooling1D()(particles_encoded)\n",
    "        jet_encoded = get_encoding(self._input_jet, self.projection_dim)\n",
    "        representation = layers.Dense(self.projection_dim,activation='gelu')(representation+jet_encoded)\n",
    "        outputs_dm_pred = layers.Dense(num_classes, activation=\"softmax\")(representation)\n",
    "        \n",
    "        self.decaymode_head = keras.Model(\n",
    "            inputs=[self._input_features, self._input_points, self._input_mask, self._input_jet, self._input_time],\n",
    "            outputs=[outputs_dm_pred], name=\"decaymode_head\"\n",
    "        )\n",
    "\n",
    "    def PET_body(self,\n",
    "                 input_features,\n",
    "                 input_points,\n",
    "                 input_mask,\n",
    "                 input_time,\n",
    "                 local, K,num_local,\n",
    "                 talking_head,\n",
    "                 ):\n",
    "            \n",
    "        #Randomly drop features not present in other datasets\n",
    "        encoded = RandomDrop(self.feature_drop if  'all' in self.mode else 0.0,num_skip=self.num_keep)(input_features)                        \n",
    "        encoded = get_encoding(encoded,self.projection_dim)\n",
    "\n",
    "        time = FourierProjection(input_time,self.projection_dim)\n",
    "        time = tf.tile(time[:,None, :], [1,tf.shape(encoded)[1], 1])*input_mask\n",
    "        time = layers.Dense(2*self.projection_dim,activation='gelu',use_bias=False)(time)\n",
    "        scale,shift = tf.split(time,2,-1)\n",
    "        \n",
    "        encoded = encoded*(1.0+scale) + shift\n",
    "        \n",
    "        if local:\n",
    "            coord_shift = tf.multiply(999., tf.cast(tf.equal(input_mask, 0), dtype='float32'))        \n",
    "            points = input_points[:,:,:2]\n",
    "            local_features = input_features\n",
    "            for _ in range(num_local):\n",
    "                local_features = get_neighbors(coord_shift+points,local_features,self.projection_dim,K)\n",
    "                points = local_features\n",
    "                \n",
    "            encoded = layers.Add()([local_features,encoded])\n",
    "\n",
    "        skip_connection = encoded\n",
    "        for i in range(self.num_layers):\n",
    "            x1 = layers.GroupNormalization(groups=1)(encoded)\n",
    "            if talking_head:\n",
    "                updates, _ = TalkingHeadAttention(self.projection_dim, self.num_heads, 0.0)(x1)\n",
    "            else:\n",
    "                updates = layers.MultiHeadAttention(num_heads=self.num_heads,\n",
    "                                                    key_dim=self.projection_dim//self.num_heads)(x1,x1)\n",
    "\n",
    "            if self.layer_scale:\n",
    "                updates = LayerScale(self.layer_scale_init, self.projection_dim)(updates,input_mask)\n",
    "            updates = StochasticDepth(self.drop_probability)(updates)\n",
    "            x2 = layers.Add()([updates,encoded])\n",
    "            x3 = layers.GroupNormalization(groups=1)(x2)\n",
    "            x3 = layers.Dense(2*self.projection_dim,activation=\"gelu\")(x3)\n",
    "            x3 = layers.Dropout(self.dropout)(x3)\n",
    "            x3 = layers.Dense(self.projection_dim)(x3)\n",
    "            if self.layer_scale:\n",
    "                x3 = LayerScale(self.layer_scale_init, self.projection_dim)(x3,input_mask)\n",
    "            x3 = StochasticDepth(self.drop_probability)(x3)\n",
    "            encoded = layers.Add()([x3,x2])*input_mask\n",
    "        return encoded + skip_connection\n",
    "    \n",
    "    def call(self, x):\n",
    "        ret = self.decaymode_head([\n",
    "            x[\"input_features\"], x[\"input_points\"], x[\"input_mask\"], x[\"input_jet\"], x[\"input_time\"]\n",
    "        ])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm_direct = TransformerModel(False, 13, 4, 16)\n",
    "x, y = prepare_data(0, 256)\n",
    "model_dm_direct(x)\n",
    "\n",
    "model_dm_bb = TransformerModel(True, 13, 4, 16)\n",
    "x, y = prepare_data(0, 256)\n",
    "model_dm_bb(x)\n",
    "\n",
    "#Load the weights of the backbone, freeze\n",
    "model_dm_bb.backbone.set_weights(model.body.weights)\n",
    "model_dm_bb.backbone.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c8cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm_direct.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd0f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(0, 400000)\n",
    "X_val, y_val = prepare_data(400000, 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm_direct.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    ")\n",
    "history = model_dm_direct.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223854c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f2ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm_bb = TransformerModel(True, 13, 4, 16)\n",
    "x, y = prepare_data(0, 256)\n",
    "model_dm_bb(x)\n",
    "#model_dm_bb.backbone.set_weights(model.body.weights)\n",
    "model_dm_bb.backbone.trainable = True\n",
    "model_dm_bb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm_bb.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    ")\n",
    "history2 = model_dm_bb.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cdba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history[\"loss\"])\n",
    "plt.plot(history2.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861f684-a781-4023-9e18-583914639a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm_bb_cp = TransformerModel(True, 13, 4, 16)\n",
    "x, y = prepare_data(0, 256)\n",
    "model_dm_bb_cp(x)\n",
    "model_dm_bb_cp.backbone.set_weights(model.body.weights)\n",
    "model_dm_bb_cp.backbone.trainable = True\n",
    "model_dm_bb_cp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf1155-9718-47ce-ada7-abb463dd0765",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm_bb_cp.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    ")\n",
    "history3 = model_dm_bb_cp.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121cede-7a84-4f4a-bf2e-e67fd4ace968",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history3.history[\"loss\"])\n",
    "plt.plot(history3.history[\"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e0d881-be05-4ac3-b30e-2a3cdd69a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"direct\", color=\"black\")\n",
    "plt.plot(history2.history[\"loss\"], label=\"OmniLearn naive\", color=\"red\")\n",
    "plt.plot(history3.history[\"loss\"], label=\"OmniLearn checkpoint\", color=\"orange\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"train loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b90291",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"val_loss\"], label=\"direct\", color=\"black\")\n",
    "plt.plot(history2.history[\"val_loss\"], label=\"OmniLearn naive\", color=\"red\")\n",
    "plt.plot(history3.history[\"val_loss\"], label=\"OmniLearn checkpoint\", color=\"orange\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"validation loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

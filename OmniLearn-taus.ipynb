{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bc0944-e298-4d08-b42e-8fcafda114fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"scripts\")\n",
    "import utils\n",
    "from PET import PET\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019bea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 512\n",
    "LR = 1e-5\n",
    "\n",
    "NTRAIN = 100000\n",
    "NVAL = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a2452",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a64a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward\n",
    "import vector\n",
    "\n",
    "def to_p4(p4_obj):\n",
    "    return vector.awk(\n",
    "        awkward.zip(\n",
    "            {\n",
    "                \"mass\": p4_obj.tau,\n",
    "                \"x\": p4_obj.x,\n",
    "                \"y\": p4_obj.y,\n",
    "                \"z\": p4_obj.z,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "def deltaphi(phi1, phi2):\n",
    "    diff = phi1 - phi2\n",
    "    return np.arctan2(np.sin(diff), np.cos(diff))\n",
    "\n",
    "def deltar(eta1, phi1, eta2, phi2):\n",
    "    deta = eta1 - eta2\n",
    "    dphi = deltaphi(phi1, phi2)\n",
    "    return np.sqrt(deta**2 + dphi**2)\n",
    "\n",
    "#initialize weights of model1 from weights of model2.\n",
    "#ignore layers that don't match in shape.\n",
    "def set_matching_weights(model1, model2):\n",
    "    il1 = len(model1.layers)\n",
    "    il2 = len(model2.layers)\n",
    "    maxl = max(il1, il1)\n",
    "    for il in range(maxl):\n",
    "        if il<il1 and il<il2:\n",
    "            if len(model1.layers[il].weights)>0:\n",
    "                if len(model1.layers[il].weights) == len(model2.layers[il].weights):\n",
    "                    weights_match = True\n",
    "                    for w1, w2 in zip(model1.layers[il].weights, model2.layers[il].weights):\n",
    "                        if w1.shape != w2.shape:\n",
    "                            weights_match = False\n",
    "                    if weights_match:\n",
    "                        print(model1.layers[il].name, model2.layers[il].name, [w.shape.as_list() for w in model1.layers[il].weights])\n",
    "                        model1.layers[il].set_weights(model2.layers[il].weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c642ee",
   "metadata": {},
   "source": [
    "Define the backbone model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6befe447-e662-4229-a30f-73902740eb4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Define original model to get the weights\n",
    "model = PET(\n",
    "    num_feat=13,\n",
    "    num_jet=4,\n",
    "    num_classes=10, #this is the number of target classes the original model was trained with. not relevant for us.\n",
    "    local=True,\n",
    "    num_layers=8,\n",
    "    drop_probability=0,\n",
    "    simple=False,\n",
    "    layer_scale=True,\n",
    "    talking_head=False,\n",
    "    mode=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76249689",
   "metadata": {},
   "source": [
    "Run the backbone model on dummy data to initialize weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e701f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {}\n",
    "x[\"input_features\"] = tf.zeros((32, 100, 13))\n",
    "x[\"input_points\"] = tf.zeros((32, 100, 2))\n",
    "x[\"input_mask\"] = tf.zeros((32, 100))\n",
    "x[\"input_jet\"] = tf.zeros((32, 4)) #how many features per jet\n",
    "x[\"input_time\"] = tf.zeros((32, 1)) #this is not used\n",
    "\n",
    "model(x)\n",
    "out = model.body(x)\n",
    "print(len(out), out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d69405f",
   "metadata": {},
   "source": [
    "Load the weights of the backbone model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca000ed4-2aa9-4a77-8de0-2a58b289eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.load_weights(\"checkpoints/PET_jetclass_8_local_layer_scale_token_baseline_all.weights.h5\", by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ae706",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = awkward.from_parquet(\"zh.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reco_cand_p4s = to_p4(data[\"reco_cand_p4s\"])\n",
    "reco_jet_p4s = to_p4(data[\"reco_jet_p4s\"])\n",
    "\n",
    "delta_eta = reco_cand_p4s.eta - reco_jet_p4s.eta\n",
    "delta_phi = deltaphi(reco_cand_p4s.phi, reco_jet_p4s.phi)\n",
    "log_pt = np.log(reco_cand_p4s.pt)\n",
    "log_e = np.log(reco_cand_p4s.energy)\n",
    "log_ptjet = np.log(1 - reco_cand_p4s.pt/reco_jet_p4s.pt)\n",
    "log_ejet = np.log(1 - reco_cand_p4s.energy/reco_jet_p4s.energy)\n",
    "delta_r = deltar(reco_cand_p4s.eta, reco_cand_p4s.phi, reco_jet_p4s.eta, reco_jet_p4s.phi)\n",
    "charge = data[\"reco_cand_charge\"]\n",
    "is_ele = np.abs(data[\"reco_cand_pdg\"])==11\n",
    "is_mu = np.abs(data[\"reco_cand_pdg\"])==13\n",
    "is_photon = np.abs(data[\"reco_cand_pdg\"])==22\n",
    "is_chhad = np.abs(data[\"reco_cand_pdg\"])==210\n",
    "is_nhad = np.abs(data[\"reco_cand_pdg\"])==130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e7a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_size = 32 #max number of particles per jet\n",
    "fill_val = 0 #fill value of padded data\n",
    "\n",
    "#create particle array in the shape [njets, pad_size, 13]\n",
    "vals = [\n",
    "    awkward.to_numpy(\n",
    "        awkward.fill_none(\n",
    "            awkward.pad_none(\n",
    "                x, pad_size, clip=True), fill_val\n",
    "        )\n",
    "    ) for x in [delta_eta, delta_phi, log_pt, log_e, log_ptjet, log_ejet, delta_r, charge, is_ele, is_mu, is_photon, is_chhad, is_nhad]\n",
    "]\n",
    "particles = np.stack(vals, axis=-1)\n",
    "particles[np.isnan(particles)] = 0\n",
    "particles[np.isinf(particles)] = 0\n",
    "particles_mask = (~awkward.to_numpy(awkward.pad_none(delta_eta, pad_size, clip=True)).mask).astype(np.float32)\n",
    "\n",
    "#normalize particles\n",
    "means_particle = particles[np.squeeze(particles_mask==1)].mean(axis=0)\n",
    "stds_particle = particles[np.squeeze(particles_mask==1)].std(axis=0)\n",
    "stds_particle[stds_particle==0] = 1\n",
    "particles = (particles - means_particle)/stds_particle\n",
    "\n",
    "#create jet array in the shape [njets, 4]\n",
    "jets = awkward.to_numpy(np.stack([\n",
    "    reco_jet_p4s.pt,\n",
    "    reco_jet_p4s.eta,\n",
    "    reco_jet_p4s.mass,\n",
    "    awkward.num(reco_cand_p4s)], axis=-1)\n",
    ")\n",
    "jets[np.isnan(jets)] = 0\n",
    "jets[np.isinf(jets)] = 0\n",
    "\n",
    "#normalize jets\n",
    "means_jet = jets.mean(axis=0)\n",
    "stds_jet = jets.std(axis=0)\n",
    "stds_jet[stds_jet==0] = 1\n",
    "jets = (jets - means_jet)/stds_jet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7381908",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_particle, stds_particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f89f1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "means_jet, stds_jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4486af",
   "metadata": {},
   "outputs": [],
   "source": [
    "particles.shape, particles_mask.shape, jets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313a47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = awkward.to_numpy(data[\"gen_jet_tau_decaymode\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc147e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(start, stop):\n",
    "    x = {}\n",
    "    x[\"input_features\"] = particles[start:stop]\n",
    "    x[\"input_points\"] = particles[start:stop, :, :2]\n",
    "    x[\"input_mask\"] = np.expand_dims(particles_mask[start:stop], axis=-1)\n",
    "    x[\"input_jet\"] = jets[start:stop]\n",
    "    x[\"input_time\"] = np.zeros((stop-start, 1))\n",
    "    y = targets[start:stop]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from layers import StochasticDepth, TalkingHeadAttention, LayerScale, RandomDrop\n",
    "\n",
    "def get_encoding(x, projection_dim, use_bias=True):\n",
    "    x = layers.Dense(2*projection_dim, use_bias=use_bias, activation='gelu')(x)\n",
    "    x = layers.Dense(projection_dim, use_bias=use_bias, activation='gelu')(x)\n",
    "    return x\n",
    "\n",
    "def FourierProjection(x,projection_dim,num_embed=64):    \n",
    "    half_dim = num_embed // 2\n",
    "    emb = tf.math.log(10000.0) / (half_dim - 1)\n",
    "    emb = tf.cast(emb,tf.float32)\n",
    "    freq = tf.exp(-emb* tf.range(start=0, limit=half_dim, dtype=tf.float32))\n",
    "\n",
    "\n",
    "    angle = x*freq*1000.0\n",
    "    embedding = tf.concat([tf.math.sin(angle),tf.math.cos(angle)],-1)*x\n",
    "    embedding = layers.Dense(2*projection_dim,activation=\"swish\",use_bias=False)(embedding)\n",
    "    embedding = layers.Dense(projection_dim,activation=\"swish\",use_bias=False)(embedding)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "def knn(num_points, k, topk_indices, features):\n",
    "    # topk_indices: (N, P, K)\n",
    "    # features: (N, P, C)    \n",
    "    batch_size = tf.shape(features)[0]\n",
    "\n",
    "    batch_indices = tf.reshape(tf.range(batch_size), (-1, 1, 1))\n",
    "    batch_indices = tf.tile(batch_indices, (1, num_points, k))\n",
    "    indices = tf.stack([batch_indices, topk_indices], axis=-1)\n",
    "    return tf.gather_nd(features, indices)\n",
    "\n",
    "def get_neighbors(points,features,projection_dim,K):\n",
    "    drij = pairwise_distance(points)  # (N, P, P)\n",
    "    _, indices = tf.nn.top_k(-drij, k=K + 1)  # (N, P, K+1)\n",
    "    indices = indices[:, :, 1:]  # (N, P, K)\n",
    "    knn_fts = knn(tf.shape(points)[1], K, indices, features)  # (N, P, K, C)\n",
    "    knn_fts_center = tf.broadcast_to(tf.expand_dims(features, 2), tf.shape(knn_fts))\n",
    "    local = tf.concat([knn_fts-knn_fts_center,knn_fts_center],-1)\n",
    "    local = layers.Dense(2*projection_dim,activation='gelu')(local)\n",
    "    local = layers.Dense(projection_dim,activation='gelu')(local)\n",
    "    local = tf.reduce_mean(local,-2)\n",
    "    \n",
    "    return local\n",
    "\n",
    "def pairwise_distance(point_cloud):\n",
    "    r = tf.reduce_sum(point_cloud * point_cloud, axis=2, keepdims=True)\n",
    "    m = tf.matmul(point_cloud, point_cloud, transpose_b = True)\n",
    "    D = r - 2 * m + tf.transpose(r, perm=(0, 2, 1)) + 1e-5\n",
    "    return D\n",
    "\n",
    "\n",
    "class TransformerModel(keras.Model):\n",
    "    def __init__(self,\n",
    "                 use_backbone,\n",
    "                 num_feat,\n",
    "                 num_jet,      \n",
    "                 num_classes=2):\n",
    "        \n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.projection_dim = model.projection_dim\n",
    "        self.num_heads = model.num_heads\n",
    "        self.num_classes = num_classes\n",
    "        self.class_activation = \"softmax\"\n",
    "        \n",
    "        self.feature_drop = model.feature_drop\n",
    "        self.num_keep = model.num_keep\n",
    "        self.mode = model.mode\n",
    "        self.num_layers = model.num_layers\n",
    "        self.layer_scale = model.layer_scale\n",
    "        self.layer_scale_init = model.layer_scale_init\n",
    "        self.drop_probability = model.drop_probability\n",
    "        self.dropout = model.dropout\n",
    "        self.num_class_layers = 2\n",
    "        \n",
    "        self._input_features = layers.Input(shape=(None, num_feat), name='input_features')\n",
    "        self._input_points = layers.Input(shape=(None, 2), name='input_points')\n",
    "        self._input_mask = layers.Input(shape=(None, 1), name='input_mask')\n",
    "        self._input_jet = layers.Input((num_jet, ),name='input_jet')\n",
    "        self._input_time = layers.Input((None, ),name='input_time')\n",
    "\n",
    "        if use_backbone:\n",
    "            self.backbone_body = self.PET_body(\n",
    "                self._input_features,\n",
    "                self._input_points,\n",
    "                self._input_mask,\n",
    "                self._input_time,\n",
    "                True,\n",
    "                self.num_classes,\n",
    "                2,\n",
    "                False\n",
    "            )\n",
    "            self.backbone = keras.Model(\n",
    "                inputs=[self._input_features, self._input_points, self._input_mask, self._input_time],\n",
    "                outputs=[self.backbone_body], name=\"backbone\"\n",
    "            )\n",
    "\n",
    "            particles_encoded = self.backbone_body\n",
    "        else:\n",
    "            particles_encoded = get_encoding(self._input_features, self.projection_dim)\n",
    "\n",
    "        classifier_out, regression_out = self.PET_classifier(\n",
    "            particles_encoded, self._input_jet, self.num_class_layers, num_jet\n",
    "        )\n",
    "        self.classifier = keras.Model(\n",
    "            inputs=[self._input_features, self._input_points, self._input_mask, self._input_jet, self._input_time],\n",
    "            outputs=[classifier_out], name=\"classifier\"\n",
    "        )\n",
    "            \n",
    "    def PET_classifier(\n",
    "            self,\n",
    "            encoded,\n",
    "            input_jet,\n",
    "            num_class_layers,\n",
    "            num_jet,\n",
    "            simple = False\n",
    "    ):\n",
    "\n",
    "        #Include event information as a representative particle\n",
    "        if simple:\n",
    "            encoded = layers.GroupNormalization(groups=1)(encoded)\n",
    "            representation = layers.GlobalAveragePooling1D()(encoded)\n",
    "            jet_encoded = get_encoding(input_jet,self.projection_dim)\n",
    "            representation = layers.Dense(self.projection_dim,activation='gelu')(representation+jet_encoded)\n",
    "            outputs_pred = layers.Dense(self.num_classes,activation=self.class_activation)(representation)\n",
    "            outputs_mse = layers.Dense(num_jet)(representation)\n",
    "        else:\n",
    "            conditional = layers.Dense(2*self.projection_dim,activation='gelu')(input_jet)\n",
    "            conditional = tf.tile(conditional[:,None, :], [1,tf.shape(encoded)[1], 1])\n",
    "            scale,shift = tf.split(conditional,2,-1)\n",
    "            encoded = encoded*(1.0 + scale) + shift\n",
    "\n",
    "            class_tokens = tf.Variable(tf.zeros(shape=(1, self.projection_dim)),trainable = True)    \n",
    "            class_tokens = tf.tile(class_tokens[None, :, :], [tf.shape(encoded)[0], 1, 1])\n",
    "                        \n",
    "            for _ in range(num_class_layers):\n",
    "                concatenated = tf.concat([class_tokens, encoded],1)\n",
    "\n",
    "                x1 = layers.GroupNormalization(groups=1)(concatenated)            \n",
    "                updates = layers.MultiHeadAttention(num_heads=self.num_heads,\n",
    "                                                    key_dim=self.projection_dim//self.num_heads)(\n",
    "                                                        query=x1[:,:1], value=x1, key=x1)\n",
    "                updates = layers.GroupNormalization(groups=1)(updates)\n",
    "                if self.layer_scale:\n",
    "                    updates = LayerScale(self.layer_scale_init, self.projection_dim)(updates)\n",
    "\n",
    "                x2 = layers.Add()([updates,class_tokens])\n",
    "                x3 = layers.GroupNormalization(groups=1)(x2)\n",
    "                x3 = layers.Dense(2*self.projection_dim,activation=\"gelu\")(x3)\n",
    "                x3 = layers.Dropout(self.dropout)(x3)\n",
    "                x3 = layers.Dense(self.projection_dim)(x3)\n",
    "                if self.layer_scale:\n",
    "                    x3 = LayerScale(self.layer_scale_init, self.projection_dim)(x3)\n",
    "                class_tokens = layers.Add()([x3,x2])\n",
    "\n",
    "\n",
    "            class_tokens = layers.GroupNormalization(groups=1)(class_tokens)\n",
    "            outputs_pred = layers.Dense(self.num_classes,activation=self.class_activation)(class_tokens[:,0])\n",
    "            outputs_mse = layers.Dense(num_jet)(class_tokens[:,0])\n",
    "\n",
    "        return outputs_pred,outputs_mse\n",
    "    \n",
    "    def PET_body(self,\n",
    "                 input_features,\n",
    "                 input_points,\n",
    "                 input_mask,\n",
    "                 input_time,\n",
    "                 local, K,num_local,\n",
    "                 talking_head,\n",
    "                 ):\n",
    "            \n",
    "        #Randomly drop features not present in other datasets\n",
    "        encoded = RandomDrop(self.feature_drop if  'all' in self.mode else 0.0,num_skip=self.num_keep)(input_features)                        \n",
    "        encoded = get_encoding(encoded,self.projection_dim)\n",
    "\n",
    "        time = FourierProjection(input_time,self.projection_dim)\n",
    "        time = tf.tile(time[:,None, :], [1,tf.shape(encoded)[1], 1])*input_mask\n",
    "        time = layers.Dense(2*self.projection_dim,activation='gelu',use_bias=False)(time)\n",
    "        scale,shift = tf.split(time,2,-1)\n",
    "        \n",
    "        encoded = encoded*(1.0+scale) + shift\n",
    "        \n",
    "        if local:\n",
    "            coord_shift = tf.multiply(999., tf.cast(tf.equal(input_mask, 0), dtype='float32'))        \n",
    "            points = input_points[:,:,:2]\n",
    "            local_features = input_features\n",
    "            for _ in range(num_local):\n",
    "                local_features = get_neighbors(coord_shift+points,local_features,self.projection_dim,K)\n",
    "                points = local_features\n",
    "                \n",
    "            encoded = layers.Add()([local_features,encoded])\n",
    "\n",
    "        skip_connection = encoded\n",
    "        for i in range(self.num_layers):\n",
    "            x1 = layers.GroupNormalization(groups=1)(encoded)\n",
    "            if talking_head:\n",
    "                updates, _ = TalkingHeadAttention(self.projection_dim, self.num_heads, 0.0)(x1)\n",
    "            else:\n",
    "                updates = layers.MultiHeadAttention(num_heads=self.num_heads,\n",
    "                                                    key_dim=self.projection_dim//self.num_heads)(x1,x1)\n",
    "\n",
    "            if self.layer_scale:\n",
    "                updates = LayerScale(self.layer_scale_init, self.projection_dim)(updates,input_mask)\n",
    "            updates = StochasticDepth(self.drop_probability)(updates)\n",
    "            x2 = layers.Add()([updates,encoded])\n",
    "            x3 = layers.GroupNormalization(groups=1)(x2)\n",
    "            x3 = layers.Dense(2*self.projection_dim,activation=\"gelu\")(x3)\n",
    "            x3 = layers.Dropout(self.dropout)(x3)\n",
    "            x3 = layers.Dense(self.projection_dim)(x3)\n",
    "            if self.layer_scale:\n",
    "                x3 = LayerScale(self.layer_scale_init, self.projection_dim)(x3,input_mask)\n",
    "            x3 = StochasticDepth(self.drop_probability)(x3)\n",
    "            encoded = layers.Add()([x3,x2])*input_mask\n",
    "        return encoded + skip_connection\n",
    "    \n",
    "    def call(self, x):\n",
    "        ret = self.classifier([\n",
    "            x[\"input_features\"], x[\"input_points\"], x[\"input_mask\"], x[\"input_jet\"], x[\"input_time\"]\n",
    "        ])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac03962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model without backbone\n",
    "model_dm_direct = TransformerModel(False, 13, 4, 16)\n",
    "x, y = prepare_data(0, 256)\n",
    "model_dm_direct(x)\n",
    "\n",
    "#model with backbone, initialization from scratch\n",
    "model_dm_bb = TransformerModel(True, 13, 4, 16)\n",
    "x, y = prepare_data(0, 256)\n",
    "model_dm_bb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c8cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm_direct.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13918160",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm_bb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd0f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = prepare_data(0, NTRAIN)\n",
    "X_val, y_val = prepare_data(NTRAIN, NTRAIN+NVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003233f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.sum(X_train[\"input_mask\"], axis=1), bins=np.linspace(0,32,33), density=1, histtype=\"step\", lw=2, label=\"train\");\n",
    "plt.hist(np.sum(X_val[\"input_mask\"], axis=1), bins=np.linspace(0,32,33), density=1, histtype=\"step\", lw=2, label=\"val\");\n",
    "plt.xlabel(\"number of particles per jet\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ed04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linspace(-10,10,100)\n",
    "\n",
    "X_train_flat = X_train[\"input_features\"][np.squeeze(X_train[\"input_mask\"]==1)]\n",
    "X_val_flat = X_val[\"input_features\"][np.squeeze(X_val[\"input_mask\"]==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3656f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    X_train_flat[:, 0],\n",
    "    bins=b, density=1, histtype=\"step\", lw=2, label=\"train\"\n",
    ");\n",
    "\n",
    "plt.hist(\n",
    "    X_val_flat[:, 0],\n",
    "    bins=b, density=1, histtype=\"step\", lw=2, label=\"val\"\n",
    ");\n",
    "plt.yscale(\"log\");\n",
    "plt.xlabel(\"feature 0: delta_eta\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    X_train_flat[:, 1],\n",
    "    bins=b, density=1, histtype=\"step\", lw=2, label=\"train\"\n",
    ");\n",
    "\n",
    "plt.hist(\n",
    "    X_val_flat[:, 1],\n",
    "    bins=b, density=1, histtype=\"step\", lw=2, label=\"val\"\n",
    ");\n",
    "plt.yscale(\"log\");\n",
    "plt.xlabel(\"feature 0: delta_phi\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dm_direct.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    ")\n",
    "history = model_dm_direct.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223854c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Direct model\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9c9119",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_init = model_dm_bb.backbone(x)\n",
    "\n",
    "model_dm_bb.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    ")\n",
    "history2 = model_dm_bb.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "encodings_init_trained = model_dm_bb.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cdba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history2.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history2.history[\"val_loss\"], label=\"val\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"OmniJet naive model\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861f684-a781-4023-9e18-583914639a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model with backbone, initialize from checkpoint\n",
    "model_dm_bb_cp = TransformerModel(True, 13, 4, 16)\n",
    "x, y = prepare_data(0, 256)\n",
    "model_dm_bb_cp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6cc0d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#set backbone weights\n",
    "outs_pre = model_dm_bb_cp.backbone(x)\n",
    "plt.scatter(\n",
    "    outs_pre[tf.squeeze(x[\"input_mask\"]==1)][:, 0],\n",
    "    outs_pre[tf.squeeze(x[\"input_mask\"]==1)][:, 1],\n",
    "    marker=\".\"\n",
    ");\n",
    "\n",
    "set_matching_weights(model_dm_bb_cp.backbone, model.body)\n",
    "\n",
    "outs_post = model_dm_bb_cp.backbone(x)\n",
    "plt.scatter(\n",
    "    outs_post[tf.squeeze(x[\"input_mask\"]==1)][:, 0],\n",
    "    outs_post[tf.squeeze(x[\"input_mask\"]==1)][:, 1],\n",
    "    marker=\".\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc66f5f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "outs_pre = model_dm_bb_cp(x)\n",
    "plt.scatter(\n",
    "    outs_pre[:, 0],\n",
    "    outs_pre[:, 1],\n",
    "    marker=\".\"\n",
    ");\n",
    "set_matching_weights(model_dm_bb_cp.classifier, model.classifier)\n",
    "\n",
    "outs_post = model_dm_bb_cp(x)\n",
    "plt.scatter(\n",
    "    outs_post[:, 0],\n",
    "    outs_post[:, 1],\n",
    "    marker=\".\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecf1155-9718-47ce-ada7-abb463dd0765",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings_before_finetune = model_dm_bb_cp.backbone(x)\n",
    "\n",
    "model_dm_bb_cp.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LR),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    ")\n",
    "history3 = model_dm_bb_cp.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "encodings_after_finetune = model_dm_bb_cp.backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73816122",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linspace(-10,10,100)\n",
    "plt.hist(\n",
    "    encodings_init[tf.squeeze(x[\"input_mask\"]==1)][:, 0],\n",
    "    bins=b,\n",
    "    label=\"OmniLearn init\", histtype=\"step\", lw=2\n",
    ");\n",
    "\n",
    "plt.hist(\n",
    "    encodings_init_trained[tf.squeeze(x[\"input_mask\"]==1)][:, 0],\n",
    "    bins=b,\n",
    "    label=\"OmniLearn init + train\", histtype=\"step\", lw=2\n",
    ");\n",
    "\n",
    "plt.hist(\n",
    "    encodings_before_finetune[tf.squeeze(x[\"input_mask\"]==1)][:, 0],\n",
    "    bins=b,\n",
    "    label=\"OmniLearn checkpoint\", histtype=\"step\", lw=2\n",
    ");\n",
    "\n",
    "plt.hist(\n",
    "    encodings_after_finetune[tf.squeeze(x[\"input_mask\"]==1)][:, 0],\n",
    "    bins=b,\n",
    "    label=\"OmniLearn checkpoint + train\", histtype=\"step\", lw=2\n",
    ");\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"encoded feature 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4866e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    encodings_init[tf.squeeze(x[\"input_mask\"]==1)][:, 0],\n",
    "    encodings_init[tf.squeeze(x[\"input_mask\"]==1)][:, 1],\n",
    "    marker=\".\",\n",
    "    alpha=0.5,\n",
    "    label=\"OmniLearn init\"\n",
    ");\n",
    "\n",
    "plt.scatter(\n",
    "    encodings_init_trained[tf.squeeze(x[\"input_mask\"]==1)][:, 0],\n",
    "    encodings_init_trained[tf.squeeze(x[\"input_mask\"]==1)][:, 1],\n",
    "    marker=\".\",\n",
    "    alpha=0.5,\n",
    "    label=\"OmniLearn init + train\"\n",
    ");\n",
    "\n",
    "plt.scatter(\n",
    "    encodings_before_finetune[tf.squeeze(x[\"input_mask\"]==1)][:, 0],\n",
    "    encodings_before_finetune[tf.squeeze(x[\"input_mask\"]==1)][:, 1],\n",
    "    marker=\".\",\n",
    "    alpha=0.5,\n",
    "    label=\"OmniLearn checkpoint\"\n",
    ");\n",
    "\n",
    "plt.scatter(\n",
    "    encodings_after_finetune[tf.squeeze(x[\"input_mask\"]==1)][:, 0],\n",
    "    encodings_after_finetune[tf.squeeze(x[\"input_mask\"]==1)][:, 1],\n",
    "    marker=\".\",\n",
    "    alpha=0.5,\n",
    "    label=\"OmniLearn checkpoint + train\"\n",
    ");\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"encoded feature 0\")\n",
    "plt.ylabel(\"encoded feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121cede-7a84-4f4a-bf2e-e67fd4ace968",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history3.history[\"loss\"], label=\"train\")\n",
    "plt.plot(history3.history[\"val_loss\"], label=\"val\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"OmniJet checkpoint model\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e0d881-be05-4ac3-b30e-2a3cdd69a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"direct\", color=\"black\")\n",
    "plt.plot(history2.history[\"loss\"], label=\"OmniLearn naive + train\", color=\"red\")\n",
    "plt.plot(history3.history[\"loss\"], label=\"OmniLearn checkpoint + train\", color=\"orange\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"train loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b90291",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"val_loss\"], label=\"direct\", color=\"black\")\n",
    "plt.plot(history2.history[\"val_loss\"], label=\"OmniLearn naive + train\", color=\"red\")\n",
    "plt.plot(history3.history[\"val_loss\"], label=\"OmniLearn checkpoint + train\", color=\"orange\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"validation loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6030ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
